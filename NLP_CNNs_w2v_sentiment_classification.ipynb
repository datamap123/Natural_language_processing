{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Copy of NLP_4_CNNs_w2v_sent_classif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIBlc9SKoXPy",
        "colab_type": "text"
      },
      "source": [
        "# CNNs with word2vec embedding for sentiment classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoWqthYoXQA",
        "colab_type": "text"
      },
      "source": [
        "In this notebook NLP is experimented with convolutional neural networks, CNNs. \n",
        "\n",
        "Tasks performed with CNNs:\n",
        "- Sentiment analysis with IMDB movie review dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWRq1lqeoXQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.width=120\n",
        "#pd.set_option('display.width',75)\n",
        "#pd.options.display.max_columns=8\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "#import nlpia\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize.casual import casual_tokenize\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import copy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN51WwH1oXQ4",
        "colab_type": "text"
      },
      "source": [
        "The alternatives are as below, let's use tf.keras here.\n",
        "- multibackend Keras \n",
        "- tf.keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8xz7IUEoXRC",
        "colab_type": "code",
        "outputId": "e92f5aae-0599-4c1a-b752-98dad557b810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.__version__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKB6EFREoXRm",
        "colab_type": "code",
        "outputId": "63301206-766f-4fdb-a3db-e28e06732267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG4QBYdNoXSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSrn6YmoXSj",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis with CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oIH-M1LoXTQ",
        "colab_type": "text"
      },
      "source": [
        "Stanford AI department provides dataset for IMDB moview reviews in https://ai.stanford.edu/%7eamaas/data/sentiment\n",
        "- This is a dataset for binary sentiment classification  \n",
        "- 25,000 highly polarised movie reviews for training, and 25,000 for testing. \n",
        "- There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.   \n",
        "\n",
        "Published papers based on this dataset:\n",
        "- Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhn83JDVoXTZ",
        "colab_type": "text"
      },
      "source": [
        "### a) Load and preprocess the imdb data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy4i-6CYoXTo",
        "colab_type": "text"
      },
      "source": [
        "Download the original dataset. We'll use the train directory only, which contains text files in pos and neg folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qAxvfqXoXT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "def preprocess_data(filepath):\n",
        "    positive_path=os.path.join(filepath,'pos')\n",
        "    negative_path=os.path.join(filepath,'neg')\n",
        "    pos_label=1\n",
        "    neg_label=0\n",
        "    dataset=[]\n",
        "    for filename in glob.glob(os.path.join(positive_path,'*.txt')):\n",
        "        with open(filename,'r') as f:\n",
        "            dataset.append((pos_label,f.read()))\n",
        "    for filename in glob.glob(os.path.join(negative_path,'*.txt')):\n",
        "        with open(filename,'r') as f:\n",
        "            dataset.append((neg_label,f.read()))  \n",
        "    shuffle(dataset)\n",
        "    return(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcJKpkno7fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=preprocess_data('/content/gdrive/My Drive/ColabFolder/imdb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84eE1aLcTtfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZl_Qyf8lNLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/imdb_dataset','wb') as fp:\n",
        "  pickle.dump(dataset,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3p46oFTG4yS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/imdb_dataset','rb') as fp:\n",
        "  dataset=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3gCp_Ph462j",
        "colab_type": "code",
        "outputId": "26a0172e-cae5-4fa8-930c-5bee924d0c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0,\n",
              " 'The Comeback starts off looking promising, with a brutal death scene by a mask wearing killer. The mask itself is pretty cool too, and looks almost identical to the one used in the 1990\\'s slasher film \"Granny\". From then on the film is mostly boring. We get a few more deaths, which again are good, but there\\'s not enough of them. The reason the deaths are so good is because they are frenzied and bloody. The story behind the film is actually rather interesting and would have worked very well had it not been so boring for the most part. <br /><br />I would avoid this unless you\\'re a die-hard collector - there\\'s not enough here to even make it an average slasher flick.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulxQ5K8C6KPs",
        "colab_type": "code",
        "outputId": "f9adbf5e-ad03-4adb-fc9a-c0c67e827ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "dataset[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'I found this movie hilarious. The spoofs on other popular movies of that time were some of the funniest I have seen in this sort of movie. Give it a try. If you saw the movies that this movie is spoofing, and you get the humor, you should enjoy the movie.<br /><br />I (and the others who watched the movie with me) felt the funniest part in the movie (this is not a spoiler because I will NOT tell you what actually happens) was a scene with the \"flashy thingy\" from MIB. When they first discover the device and do not know what it is does... and then again later in the movie... you\\'ll understand when you get there.<br /><br />My only complaint about the movie is that I have never been able to find it in DVD so that I could buy a copy.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ZQ8MTX8DFN",
        "colab_type": "code",
        "outputId": "55e02b73-59d1-471e-92b9-8e1f58455b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-7Nb-0RoXU6",
        "colab_type": "text"
      },
      "source": [
        "### b) Tokenize and vectorise the imdb data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U150YqM88ab6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHt7XOzw8a0J",
        "colab_type": "code",
        "outputId": "9551d0fa-56fb-47fb-ecc7-3b480fbe82ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "wv=api.load('word2vec-google-news-300')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PuXlCULoXWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer=TreebankWordTokenizer()\n",
        "    vectorized_data=[]\n",
        "    expected=[]\n",
        "    for sample in dataset:\n",
        "        tokens=tokenizer.tokenize(sample[1])\n",
        "        sample_vecs=[]\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(wv[token])\n",
        "            except KeyError:\n",
        "                pass # No matching token in the Google w2v vocab\n",
        "        vectorized_data.append(sample_vecs)\n",
        "    return vectorized_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bB0MHooXWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_expected(dataset):\n",
        "    \"\"\"Peel off the target values from the dataset\"\"\"\n",
        "    expected=[]\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUBtdgnIoXXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pass the imdb data into the two functions\n",
        "vectorized_data=tokenize_and_vectorize(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDe7kV_95I0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/vectorized_data','wb') as fp:\n",
        "  pickle.dump(vectorized_data,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMXgjtE5GCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expected=collect_expected(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYQVtdg37g3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/expected','wb') as fp:\n",
        "  pickle.dump(expected,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkf349gqPFQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/vectorized_data','rb') as fp:\n",
        "  vectorized_data=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C5Qc7z-S5m_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "vectorized_data=vectorized_data[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usbOhSZRPFqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/expected','rb') as fp:\n",
        "  expected=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GW9Zg4xRqf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "expected=expected[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G2KExP8oXXg",
        "colab_type": "text"
      },
      "source": [
        "### c) Create training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah1mUx_2oXXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data is already shuffled so the splitting can be done through slicing.\n",
        "split_point=int(len(vectorized_data)*0.8)\n",
        "x_train=vectorized_data[:split_point]\n",
        "x_test=vectorized_data[split_point:]\n",
        "y_train=expected[:split_point]\n",
        "y_test=expected[split_point:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-1ejmdpoXYI",
        "colab_type": "text"
      },
      "source": [
        "### d) Padding and truncating the token sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ElQ49sLoXYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN parameters\n",
        "maxlen=400 # max length of the sequences (to be padded/truncated to this length)\n",
        "batch_size=32 # number of samples before backpropagating and updating the weights\n",
        "embedding_dims=300\n",
        "filters=250\n",
        "kernel_size=3\n",
        "hidden_dims=250 # number of neurons in the dense network at the end of the chain\n",
        "epochs=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESZGPh8CoXYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_trunc(data,maxlen):\n",
        "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlength\"\"\"\n",
        "    new_data=[]\n",
        "    # create a vector of 0s the length of word embedding vectors\n",
        "    zero_vector=[]\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            temp=sample[:maxlen]\n",
        "        elif len(sample)<maxlen:\n",
        "            temp=sample\n",
        "            # Append the appropriate number of zero_vectors to the list\n",
        "            additional_elems=maxlen-len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp=sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9qn5UJ0oXY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alternative way to define the pad_trunc function\n",
        "#def pad_trunc_2(data,maxlen,emb_dim):\n",
        "#    new_data=[smp[:maxlen]+[[0.]*emb_dim]*(maxlen-len(smp)) for smp in data]\n",
        "#    return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1caIxjjoXZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the padding and truncation\n",
        "x_train=pad_trunc(x_train,maxlen)\n",
        "x_test=pad_trunc(x_test,maxlen)\n",
        "x_train=np.reshape(x_train,(len(x_train),maxlen,embedding_dims))\n",
        "x_test=np.reshape(x_test,(len(x_test),maxlen,embedding_dims))\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-hXf6XjoXZe",
        "colab_type": "text"
      },
      "source": [
        "### e) Build the CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekzzjlntoXZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "39cc58f4-86bc-4c59-9744-46cef0f371c8"
      },
      "source": [
        "model=Sequential([\n",
        "    Conv1D(filters=filters,kernel_size=kernel_size,padding=\"valid\",activation=\"relu\",strides=1,input_shape=(maxlen,embedding_dims)),\n",
        "    GlobalMaxPooling1D(), # default n=2\n",
        "    Dense(hidden_dims,activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(1,activation=\"sigmoid\")   # If several classes to predict: Dense(num_classes, activation('sigmoid') or activation('softmax'))\n",
        "    \n",
        "])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9I3Y7iPoXZz",
        "colab_type": "code",
        "outputId": "2925320b-00c3-4945-afaf-a2cd8ace5883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 398, 250)          225250    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 250)               62750     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 288,251\n",
            "Trainable params: 288,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlRGMcUHoXaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "b537c2c0-d51a-45b4-9ba6-389fb61687fc"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) # If several classes: loss='categorical_crossentropy'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4LOMx65oXaV",
        "colab_type": "text"
      },
      "source": [
        "### f) Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t22OPC0poXaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To set the seed (to enable reproducing the same results) -> same initial random weights.\n",
        "np.random.seed(1337)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpNd6D3voXat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6be253bc-8624-48f0-bdc1-58a21a306b69"
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train,y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_test,y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3999 samples, validate on 1000 samples\n",
            "Epoch 1/2\n",
            "3999/3999 [==============================] - 10s 3ms/sample - loss: 0.5540 - acc: 0.7047 - val_loss: 0.4208 - val_acc: 0.7900\n",
            "Epoch 2/2\n",
            "3999/3999 [==============================] - 3s 873us/sample - loss: 0.2956 - acc: 0.8777 - val_loss: 0.3479 - val_acc: 0.8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c6c968f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDO_857boXa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "model_structure=model.to_json()\n",
        "with open(\"/content/gdrive/My Drive/ColabFolder/imdb/cnn_model.json\",\"w\") as json_file:\n",
        "    json_file.write(model_structure)  # this only saves the structure, not the weights\n",
        "model.save_weights(\"/content/gdrive/My Drive/ColabFolder/imdb/cnn_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLGaUg6goXbN",
        "colab_type": "text"
      },
      "source": [
        "### g) Test the model by predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiPgoh8oXbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample1=\"I hate that the dismal weather had me down for so long, \\\n",
        "when will it break! Ugh, when does happiness return? The sun is blinding \\\n",
        "and the puffy clouds are too thin. I can't wait for the weekend.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA6HvUVloXbj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "41e14d43-d9bc-4792-f46c-10a3a3285dd6"
      },
      "source": [
        "vec_list=tokenize_and_vectorize([(1,sample1)])  # target value = 1 is just dummy value, not used here\n",
        "test_vec_list=pad_trunc(vec_list,maxlen)\n",
        "test_vec=np.reshape(test_vec_list,(len(test_vec_list),maxlen,embedding_dims))\n",
        "model.predict(test_vec) # returns probability (>0.5 ->1, <0.5 ->0)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.11028806]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoc3HnfVYgdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92284727-11bc-41dc-99ab-5291613cb886"
      },
      "source": [
        "model.predict_classes(test_vec) # returns predicted class (0 or 1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Qrx4tzZeGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}