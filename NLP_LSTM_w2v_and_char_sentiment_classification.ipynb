{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "NLP_5_LSTMs_w2v_char_sent_classif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pfIeS42QPBU",
        "colab_type": "text"
      },
      "source": [
        "# NLP with LSTMs for sentiment classification (w2v and char based)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBRogjjRQPBs",
        "colab_type": "text"
      },
      "source": [
        "In this notebook NLP is experimented with long short term memory units, LSTMs. \n",
        "\n",
        "Tasks performed with LSTMs:\n",
        "- 1) Sentiment analysis for IMDB moview review dataset (with word2vec based model)\n",
        "- 2) Sentiment analysis for IMDB moview review dataset (with character based model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR_tQsFgQPCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.width=120\n",
        "#pd.set_option('display.width',75)\n",
        "#pd.options.display.max_columns=8\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize.casual import casual_tokenize\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import copy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWI5n9rwQPDE",
        "colab_type": "text"
      },
      "source": [
        "The alternatives are as below, let's use tf.keras here.\n",
        "- multibackend Keras \n",
        "- tf.keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh6cR4ppQPDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "02fe917b-6f6b-4d11-e12b-7c11d900a9e9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf3xGwdPQPEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ed92f73-6435-4bba-bdc9-85cc594622dc"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOxeeFD5QPEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are for word based model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb10Ins8QPFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10160e44-7c41-4c77-b57f-25dfb0b5eed0"
      },
      "source": [
        "# This is an addition for character based model\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT4fHo7pQPGC",
        "colab_type": "text"
      },
      "source": [
        "## 1) Sentiment analysis with LSTMs (based on word2vec embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtS9An0wQPGM",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)Stanford AI department provides dataset for IMDB moview reviews in https://ai.stanford.edu/%7eamaas/data/sentiment\n",
        "- This is a dataset for binary sentiment classification  \n",
        "- 25,000 highly polarised movie reviews for training, and 25,000 for testing. \n",
        "- There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.   \n",
        "\n",
        "Published papers based on this dataset:\n",
        "- Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAnQD3piQPGX",
        "colab_type": "text"
      },
      "source": [
        "### a) Load and preprocess the imdb data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmPX6tsyQPGj",
        "colab_type": "text"
      },
      "source": [
        "Download the original dataset. We'll use the train directory only, which contains text files in pos and neg folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr7wWtEyQPGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "def preprocess_data(filepath):\n",
        "    \"\"\"\n",
        "    This is dependent on your training data source but the idea is to have it as general as possible.\n",
        "    \"\"\"\n",
        "    positive_path=os.path.join(filepath,'pos')\n",
        "    negative_path=os.path.join(filepath,'neg')\n",
        "    pos_label=1\n",
        "    neg_label=0\n",
        "    dataset=[]\n",
        "    for filename in glob.glob(os.path.join(positive_path,'*.txt')):\n",
        "        with open(filename,'r') as f:\n",
        "            dataset.append((pos_label,f.read()))\n",
        "    for filename in glob.glob(os.path.join(negative_path,'*.txt')):\n",
        "        with open(filename,'r') as f:\n",
        "            dataset.append((neg_label,f.read()))  \n",
        "    shuffle(dataset)\n",
        "    return(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkREPgUSQPHR",
        "colab_type": "code",
        "colab": {},
        "outputId": "48b14f9f-2793-4cff-9573-f1b3af964566"
      },
      "source": [
        "dataset=preprocess_data('imdb')\n",
        "dataset[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"FUTZ is the only show preserved from the experimental theatre movement in New York in the 1960s (the origins of Off Off Broadway). Though it's not for everyone, it is a genuinely brilliant, darkly funny, even more often deeply disturbing tale about love, sex, personal liberty, and revenge, a serious morality tale even more relevant now in a time when Congress wants to outlaw gay marriage by trashing our Constitution. The story is not about being gay, though -- it's about love and sex that don't conform to social norms and therefore must be removed through violence and hate. On the surface, it tells the story of a man who falls in love with a pig, but like any great fable, it's not really about animals, it's about something bigger -- stifling conformity in America.<br /><br />The stage version won international acclaim in its original production, it toured the U.S. and Europe, and with others of its kind, influenced almost all theatre that came after it. Luckily, we have preserved here the show pretty much as it was originally conceived, with the original cast and original director, Tom O'Horgan (who also directed HAIR and Jesus Christ Superstar on Broadway).<br /><br />This is not a mainstream, easy-to-take, studio film -- this is an aggressive, unsettling, glorious, deeply emotional, wildly imaginative piece of storytelling that you'll never forget. And it just might change the way you see the world...\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VnYvrxRRxWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw0ZvhVQSp1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/imdb_dataset','wb') as fp:\n",
        "  pickle.dump(dataset,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhE-AYq8Sdlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/imdb_dataset','rb') as fp:\n",
        "  dataset=pickle.load(fp) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r90UpF6rQPHv",
        "colab_type": "text"
      },
      "source": [
        "### b) Tokenize and vectorise the imdb data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSabByZhQPH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIAv9yulQPIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "5d08971e-f8a2-4556-abfd-67a6777b4a80"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv=api.load('word2vec-google-news-300')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2km7R7cvQPI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer=TreebankWordTokenizer()\n",
        "    vectorized_data=[]\n",
        "    expected=[]\n",
        "    for sample in dataset:\n",
        "        tokens=tokenizer.tokenize(sample[1])\n",
        "        sample_vecs=[]\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(wv[token])\n",
        "            except KeyError:\n",
        "                pass # No matching token in the Google w2v vocab\n",
        "        vectorized_data.append(sample_vecs)\n",
        "    return vectorized_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EAK4lvRQPJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_expected(dataset):\n",
        "    \"\"\"Peel off the target values from the dataset\"\"\"\n",
        "    expected=[]\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lu5Zy2fQPKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pass the imdb data into the two functions\n",
        "vectorized_data=tokenize_and_vectorize(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsEzQNejTR6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/vectorized_data','wb') as fp:\n",
        "  pickle.dump(vectorized_data,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp4vlZ9uTSN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expected=collect_expected(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjblroorTv9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/expected','wb') as fp:\n",
        "  pickle.dump(expected,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPi28cS7TvwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/vectorized_data','rb') as fp:\n",
        "  vectorized_data=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cDIV7vT4Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "vectorized_data=vectorized_data[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xThtc5qQT4bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/expected','rb') as fp:\n",
        "  expected=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNgvsn-OULF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "expected=expected[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7taOfV-QPKc",
        "colab_type": "text"
      },
      "source": [
        "### c) Create training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muyur_1qQPKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data is already shuffled so the splitting can be done through slicing.\n",
        "split_point=int(len(vectorized_data)*0.8)\n",
        "x_train=vectorized_data[:split_point]\n",
        "x_test=vectorized_data[split_point:]\n",
        "y_train=expected[:split_point]\n",
        "y_test=expected[split_point:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0WiVMu8QPLC",
        "colab_type": "text"
      },
      "source": [
        "### d) Padding and truncating the token sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E9-AsJcQPLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM parameters (otherwise the same as in CNN case, except no filters nor kernels nor hidden_dims)\n",
        "maxlen=400 # max length of the sequences (to be padded/truncated to this length)\n",
        "batch_size=32 # number of samples before backpropagating and updating the weights\n",
        "embedding_dims=300\n",
        "epochs=2\n",
        "num_neurons=50 # number of neurons in each LSTM cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCZOjzqkQPLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_trunc(data,maxlen):\n",
        "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlength\"\"\"\n",
        "    new_data=[]\n",
        "    # create a vector of 0s the length of word embedding vectors\n",
        "    zero_vector=[]\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            temp=sample[:maxlen]\n",
        "        elif len(sample)<maxlen:\n",
        "            temp=sample\n",
        "            # Append the appropriate number of zero_vectors to the list\n",
        "            additional_elems=maxlen-len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp=sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtm3h_3RQPME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alternative way to define the pad_trunc function\n",
        "#def pad_trunc_2(data,maxlen,emb_dim):\n",
        "#    new_data=[smp[:maxlen]+[[0.]*emb_dim]*(maxlen-len(smp)) for smp in data]\n",
        "#    return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNSptKXsQPMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the padding and truncation\n",
        "# When using RNN recurrent neural network (either simpleRNN,LSTM or GRU cells), truncating/padding not normally needed.\n",
        "# Here we do it simply to get results that can be compared with CNN case.\n",
        "# With CNN, truncating/padding was needed since at the end there was Dense network that requires fixed length input.\n",
        "x_train=pad_trunc(x_train,maxlen)\n",
        "x_test=pad_trunc(x_test,maxlen)\n",
        "x_train=np.reshape(x_train,(len(x_train),maxlen,embedding_dims))\n",
        "x_test=np.reshape(x_test,(len(x_test),maxlen,embedding_dims))\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMeE28yHQPM1",
        "colab_type": "text"
      },
      "source": [
        "### e) Build the LSTM network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FxODJj1QPM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "f9369015-f200-483a-ee8d-db10c9ef80f0"
      },
      "source": [
        "model=Sequential([\n",
        "    LSTM(num_neurons,return_sequences=True,  # we want output at each time step\n",
        "        input_shape=(maxlen,embedding_dims)),\n",
        "    Dropout(0.2),\n",
        "    Flatten(), # Data needs to be flattened, since output from RNN/LSTM network is two dimensional: 400*50\n",
        "    Dense(1,activation=\"sigmoid\") # If several classes to predict: Dense(num_classes, activation('sigmoid')\n",
        "])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVsBbFRsQPNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "0b38593e-deeb-4181-d0c8-64f3a64f98cb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 400, 50)           70200     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 20001     \n",
            "=================================================================\n",
            "Total params: 90,201\n",
            "Trainable params: 90,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0uZNcBHQPNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "65a92233-c162-4026-91aa-d843de2a2650"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy']) # If several classes: loss='categorical_crossentropy'"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8r11pRnQPOD",
        "colab_type": "text"
      },
      "source": [
        "### f) Train and save/load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS1Q5aEtQPOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To set the seed (to enable reproducing the same results) -> same initial random weights.\n",
        "np.random.seed(1337)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUT6HeaWQPOl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "95b307d5-0e90-49a8-e388-0d232387fd0c"
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train,y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_test,y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3999 samples, validate on 1000 samples\n",
            "Epoch 1/2\n",
            "3999/3999 [==============================] - 94s 23ms/sample - loss: 0.5959 - acc: 0.6819 - val_loss: 0.4995 - val_acc: 0.7740\n",
            "Epoch 2/2\n",
            "3999/3999 [==============================] - 92s 23ms/sample - loss: 0.4512 - acc: 0.7917 - val_loss: 0.4739 - val_acc: 0.7970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d5dc406d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8SRAP2aQPO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "model_structure=model.to_json()\n",
        "with open(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_model1.json\",\"w\") as json_file:\n",
        "    json_file.write(model_structure)  # this only saves the structure, not the weights\n",
        "model.save_weights(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_weights1.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcmv-Tx_QPPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "from keras.models import model_from_json\n",
        "with open(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_model1.json\",\"r\") as json_file:\n",
        "    json_string=json_file.read()  # this only saves the structure, not the weights\n",
        "model.load_weights(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_weights1.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDuPvZMHQPP6",
        "colab_type": "text"
      },
      "source": [
        "### g) Test the model by predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVgyTI-sQPQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample1=\"I hate that the dismal weather had me down for so long, \\\n",
        "when will it break! Ugh, when does happiness return? The sun is blinding \\\n",
        "and the puffy clouds are too thin. I can't wait for the weekend.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLx2GAeQZtBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del vectorized_data\n",
        "del expected\n",
        "del x_train; del x_test; del y_train; del y_test;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtNbPDYEQPQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_list=tokenize_and_vectorize([(1,sample1)])  # target value = 1 is just dummy value, not used here\n",
        "test_vec_list=pad_trunc(vec_list,maxlen)\n",
        "test_vec=np.reshape(test_vec_list,(len(test_vec_list),maxlen,embedding_dims))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCgm-7DgakB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6a746537-d934-4374-bb75-24c686af90cb"
      },
      "source": [
        "print(\"Sample's sentiment, 1-pos, 0-neg : \")\n",
        "model.predict_classes(test_vec) # returns class "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample's sentiment, 1-pos, 0-neg : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDXpRRTh2eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "698e26c9-f9a6-420d-e873-8ef79ead384c"
      },
      "source": [
        "print(\"Raw output of sigmoid function : \")\n",
        "model.predict(test_vec) # returns probability (>0.5 ->1, <0.5 ->0)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw output of sigmoid function : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38133448]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhtIFhupQPQs",
        "colab_type": "text"
      },
      "source": [
        "### h) Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR9ha7Z3QPQw",
        "colab_type": "text"
      },
      "source": [
        "What is possible:\n",
        "- padding/truncating is in fact not generally required for LSTMs, it is only required  when e.g. CNN is combined with Dense networks, since Dense network requires fixed length input\n",
        "\n",
        "Dense network can of course combined with LSTM network also.\n",
        "- then , LSTM can be considered to create a thought vector that it inputs to Dense network\n",
        "- in that case it is good to understand which length of the thought vector would be optimised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIi0jet8QPQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimize the thought vector length, coming out of LSTM network.\n",
        "def test_len(data,maxlen):\n",
        "    total_len=truncated=exact=padded=0\n",
        "    for sample in data:\n",
        "        total_len+=len(sample)\n",
        "        if len(sample)>maxlen:\n",
        "            truncated+=1\n",
        "        elif len(sample)<maxlen:\n",
        "            padded+=1\n",
        "        else:\n",
        "            exact+=1\n",
        "    print('Padded: {}'.format(padded))\n",
        "    print('Equal: {}'.format(exact))\n",
        "    print('Truncated: {}'.format(truncated))\n",
        "    print('Avg length: {}'.format(total_len/len(data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aruoM-aDi1bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/vectorized_data','rb') as fp:\n",
        "  vectorized_data=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sNQxT42khwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "vectorized_data=vectorized_data[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kx0g6u6kvYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "6c80886a-1dcb-4610-d6ab-63a4265938e4"
      },
      "source": [
        "#dataset=preprocess_data('imdb')\n",
        "#vectorized_data=tokenize_and_vectorize(dataset)\n",
        "test_len(vectorized_data,400)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded: 4489\n",
            "Equal: 0\n",
            "Truncated: 510\n",
            "Avg length: 206.71374274854972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcE9yyV4lCIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/expected','rb') as fp:\n",
        "  expected=pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkK4BU5glB82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "expected=expected[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLKsD9_gQPRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's define max length to be 200, which is close to average length\n",
        "maxlen=200  # other parameters remain the same.\n",
        "\n",
        "# reperform the padding /truncation for x_train and x_test (not needed for y_train nor y_test)\n",
        "split_point=int(len(vectorized_data)*0.8)\n",
        "x_train=vectorized_data[:split_point]\n",
        "x_test=vectorized_data[split_point:]\n",
        "y_train=expected[:split_point]\n",
        "y_test=expected[split_point:]\n",
        "\n",
        "x_train=pad_trunc(x_train,maxlen)\n",
        "x_test=pad_trunc(x_test,maxlen)\n",
        "x_train=np.reshape(x_train,(len(x_train),maxlen,embedding_dims))\n",
        "x_test=np.reshape(x_test,(len(x_test),maxlen,embedding_dims))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjY55HXXQPRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# More optimally sized LSTM network. Structure is the same, just maxlen value is different.\n",
        "model=Sequential([\n",
        "    LSTM(num_neurons,return_sequences=True,  # we want output at each time step\n",
        "        input_shape=(maxlen,embedding_dims)),\n",
        "    Dropout(0.2),\n",
        "    Flatten(), # Data needs to be flattened, since output from RNN/LSTM network is two dimensional: 400*50\n",
        "    Dense(1,activation=\"sigmoid\") # If several classes to predict: Dense(num_classes, activation('sigmoid') or activation('softmax'))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6AnS3wVQPSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy']) # If several classes: loss='categorical_crossentropy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmg91BXwQPSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a619510b-2183-4318-afb2-13317bdca2b5"
      },
      "source": [
        "# Train the optimized LSTM network\n",
        "np.random.seed(1337)\n",
        "model.fit(x_train,y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_test,y_test))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3999 samples, validate on 1000 samples\n",
            "Epoch 1/2\n",
            "3999/3999 [==============================] - 47s 12ms/sample - loss: 0.5902 - acc: 0.6894 - val_loss: 0.4931 - val_acc: 0.7790\n",
            "Epoch 2/2\n",
            "3999/3999 [==============================] - 47s 12ms/sample - loss: 0.4645 - acc: 0.7887 - val_loss: 0.4728 - val_acc: 0.7940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8cba96bef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5kBnR4Unufm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Benefit of optimised LSTM: faster training, still accuracy didn't change very much."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZQX1cyEmlwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model with a different name\n",
        "model_structure=model.to_json()\n",
        "with open(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_model2.json\",\"w\") as json_file:\n",
        "    json_file.write(model_structure)  # this only saves the structure, not the weights\n",
        "model.save_weights(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_weights2.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlH5h_xwoNDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del x_train; del x_test; del y_train; del y_test;\n",
        "del vectorized_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8E5dkvo2T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvJIik4Vo_ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7uih323QPSk",
        "colab_type": "text"
      },
      "source": [
        "## 2) Sentiment analysis with LSTMs (based on characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBROwy79QPSp",
        "colab_type": "text"
      },
      "source": [
        "Here the sequence of characters is used for predicting the sentiment, instead of words (or word2vec embeddings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFWuG0o6QPSv",
        "colab_type": "text"
      },
      "source": [
        "### a) Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWlPEcuGqBf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('/content/gdrive/My Drive/ColabFolder/imdb/imdb_dataset','rb') as fp:\n",
        "  dataset=pickle.load(fp) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCUj91DNqLOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's reduce the dataset, otherwise there is not enough RAM available\n",
        "dataset=dataset[0:4999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkipGnl6QPS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "#dataset=preprocess_data('imdb')\n",
        "# Extract the target (y values)\n",
        "#expected=collect_expected(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1uS_42JQPTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the average sample length\n",
        "def avg_len(data):\n",
        "    total_len=0\n",
        "    for sample in data:\n",
        "        total_len+=len(sample[1])\n",
        "    return total_len/len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK2Zgn5vQPTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "618483ef-9d94-47ec-ac6f-e20fb9a3755c"
      },
      "source": [
        "avg_len(dataset) # This gives the average length for the 5000 samples that we use here\n",
        "# result with the whole dataset: 1325.1. Thus character based LSTM network will be much longer compared to word based."
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1334.4052810562112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r42BHIsjQPTp",
        "colab_type": "text"
      },
      "source": [
        "### b) Insert UNK (unknown) characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-vOUt_7QPTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the strings for a character based model\n",
        "# UNK is used as a single character for everything that doesn't match the VALID list (could be e.g. HTML tags)\n",
        "def clean_data(data):\n",
        "    \"\"\"Shift to lower case, replace unknowns with UNK and listify\"\"\"\n",
        "    new_data=[]\n",
        "    VALID='abcdefghijklmnopqrstuvwyz0123456789\"\\'?!.,:; '\n",
        "    for sample in data:\n",
        "        new_sample=[]\n",
        "        for char in sample[1].lower():\n",
        "            if char in VALID:\n",
        "                new_sample.append(char)\n",
        "            else:\n",
        "                new_sample.append('UNK')\n",
        "        new_data.append(new_sample)\n",
        "    return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35hdxzW2QPUF",
        "colab_type": "text"
      },
      "source": [
        "### c) Pad and truncate the character sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIYypwPFQPUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's use maxlen that is a bit higher than the avg length.\n",
        "def char_pad_trunc(data,maxlen=1500):\n",
        "    \"\"\"Truncate to maxlen or add in PAD tokens\"\"\"\n",
        "    new_dataset=[]\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            new_data=sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            pads=maxlen-len(sample)\n",
        "            new_data=sample+['PAD']*pads\n",
        "        else:\n",
        "            new_data=sample\n",
        "        new_dataset.append(new_data)\n",
        "    return new_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6htTVNfQPUg",
        "colab_type": "text"
      },
      "source": [
        "### d) Create character based model vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDGhUjwHQPUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create characters mapped to integer indices, and vice versa.\n",
        "def create_dicts(data):\n",
        "    \"\"\"Modified from Keras LSTM example\"\"\"\n",
        "    chars=set()\n",
        "    for sample in data:\n",
        "        chars.update(set(sample))\n",
        "    char_indices=dict((c,i) for i,c in enumerate(chars))\n",
        "    indices_char=dict((i,c) for i,c in enumerate(chars))\n",
        "    return char_indices,indices_char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SryJS2cyQPVC",
        "colab_type": "text"
      },
      "source": [
        "### e) One-hot encoding for characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjSPfyLoQPVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot_encode(dataset,char_indices,maxlen=1500):\n",
        "    \"\"\"\n",
        "    One-hot encode the tokens\n",
        "    \n",
        "    Args:\n",
        "        dataset list of lists of tokens\n",
        "        char_indices dict of (key=character,value=index)\n",
        "        maxlen int Length of each sample\n",
        "    Return:\n",
        "        np array of shape (samples,tokens,encoding length)\n",
        "    \"\"\"\n",
        "    X=np.zeros((len(dataset),maxlen,len(char_indices.keys())))\n",
        "    for i,sentence in enumerate(dataset):\n",
        "        for t,char in enumerate(sentence):\n",
        "            X[i,t,char_indices[char]]=1\n",
        "    return X                               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTkNotbzQPVd",
        "colab_type": "text"
      },
      "source": [
        "### f) Preprocess the data (clean,pad/trunc,vocabulary,one-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZckjpaDQPVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listified_data=clean_data(dataset) #Insert also the UNKs\n",
        "common_length_data=char_pad_trunc(listified_data,maxlen=1500)\n",
        "char_indices,indices_char=create_dicts(common_length_data)\n",
        "encoded_data=onehot_encode(common_length_data,char_indices,1500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rIpXN23QPVy",
        "colab_type": "text"
      },
      "source": [
        "### g) Create training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOwT6FzoQPV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_point=int(len(encoded_data)*0.8)\n",
        "x_train=encoded_data[:split_point]\n",
        "x_test=encoded_data[split_point:]\n",
        "y_train=expected[:split_point]\n",
        "y_test=expected[split_point:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkEmL-A0QPWM",
        "colab_type": "text"
      },
      "source": [
        "### h) Build a character-based LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgu8uRcVQPWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_neurons=40\n",
        "maxlen=1500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWu8QW0zQPWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Otherwise the same LSTM model except new values for num_neurons and maxlen\n",
        "# Note, also the second value in input_shape is no longer word2vec dimension of 300\n",
        "model=Sequential([\n",
        "    LSTM(num_neurons,return_sequences=True,  # we want output at each time step\n",
        "        input_shape=(maxlen,len(char_indices.keys()))),  # length of sequences * length of one-hot encoding\n",
        "    Dropout(0.2),\n",
        "    Flatten(), # Data needs to be flattened, since output from RNN/LSTM network is two dimensional: 400*50\n",
        "    Dense(1,activation=\"sigmoid\") # If several classes to predict: Dense(num_classes, activation('sigmoid') or activation('softmax'))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyNGX-cxQPWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy']) # If several classes: loss='categorical_crossentropy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BArwR6sUQPXF",
        "colab_type": "text"
      },
      "source": [
        "### i) Train the character based LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7jv-WCHQPXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=32\n",
        "epochs=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX1jB4w2QPXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "3148fec0-e8db-43a0-a3db-028a2b964793"
      },
      "source": [
        "np.random.seed(1337)\n",
        "model.fit(x_train,y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_test,y_test))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3999 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "3999/3999 [==============================] - 321s 80ms/sample - loss: 0.7350 - acc: 0.5221 - val_loss: 0.6992 - val_acc: 0.5060\n",
            "Epoch 2/10\n",
            "3999/3999 [==============================] - 320s 80ms/sample - loss: 0.6119 - acc: 0.7037 - val_loss: 0.7147 - val_acc: 0.5170\n",
            "Epoch 3/10\n",
            "3999/3999 [==============================] - 319s 80ms/sample - loss: 0.5004 - acc: 0.7912 - val_loss: 0.7249 - val_acc: 0.5470\n",
            "Epoch 4/10\n",
            "3999/3999 [==============================] - 318s 80ms/sample - loss: 0.4021 - acc: 0.8527 - val_loss: 0.7742 - val_acc: 0.5500\n",
            "Epoch 5/10\n",
            "3999/3999 [==============================] - 319s 80ms/sample - loss: 0.3197 - acc: 0.8967 - val_loss: 0.8318 - val_acc: 0.5560\n",
            "Epoch 6/10\n",
            "3999/3999 [==============================] - 319s 80ms/sample - loss: 0.2442 - acc: 0.9320 - val_loss: 0.9131 - val_acc: 0.5520\n",
            "Epoch 7/10\n",
            "3999/3999 [==============================] - 319s 80ms/sample - loss: 0.1868 - acc: 0.9510 - val_loss: 1.0165 - val_acc: 0.5490\n",
            "Epoch 8/10\n",
            "3999/3999 [==============================] - 319s 80ms/sample - loss: 0.1390 - acc: 0.9685 - val_loss: 1.1507 - val_acc: 0.5350\n",
            "Epoch 9/10\n",
            "3999/3999 [==============================] - 318s 79ms/sample - loss: 0.1012 - acc: 0.9810 - val_loss: 1.2610 - val_acc: 0.5360\n",
            "Epoch 10/10\n",
            "3999/3999 [==============================] - 317s 79ms/sample - loss: 0.0740 - acc: 0.9915 - val_loss: 1.3875 - val_acc: 0.5290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d524bd898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvQksdOk605c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obviously the training set was too small for this case. \n",
        "# Since accuracy for training set is 99 % and only 52.9 % for the validation set, it clearly indicates overfitting."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPY8PaBpQPXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model with a different name\n",
        "model_structure=model.to_json()\n",
        "with open(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_model3.json\",\"w\") as json_file:\n",
        "    json_file.write(model_structure)  # this only saves the structure, not the weights\n",
        "model.save_weights(\"/content/gdrive/My Drive/ColabFolder/imdb/lstm_weights3.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}