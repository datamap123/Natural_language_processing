{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for text generation (char based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook NLP is experimented with long short term memory units, LSTMs. \n",
    "\n",
    "Tasks performed with LSTMs:\n",
    "- Generating text รก la Shakespeare (with character based model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.width=120\n",
    "#pd.set_option('display.width',75)\n",
    "#pd.options.display.max_columns=8\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import nlpia\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternatives are as below, let's use tf.keras here.\n",
    "- multibackend Keras \n",
    "- tf.keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in the task \n",
    "- to learn to predict 41st character, based on 40 characters that came before that.\n",
    "- for this purpose the source text is chopped up into data samples, each with the fixed size of 40 characters.\n",
    "- the samples are taken as follows: take 40 chars from beginning, then move to 3rd character, take 40 from there etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Load Project Gutenberg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Load three Shakespeare texts and preprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus length: 375542 total chars: 50'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=''\n",
    "for txt in gutenberg.fileids():\n",
    "    if 'shakespeare' in txt:\n",
    "        text+=gutenberg.raw(txt).lower()  # load the raw text based on the selected fileid and concatenate it to text string\n",
    "chars=sorted(list(set(text)))  # create a set of all characters, turn it into a list, and then sort it.\n",
    "char_indices=dict((c,i) for i,c in enumerate(chars))  \n",
    "indices_char=dict((i,c) for i,c in enumerate(chars))\n",
    "'corpus length: {} total chars: {}'.format(len(text),len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of julius caesar by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter flauius, murellus, and certaine commoners ouer the stage.\n",
      "\n",
      "  flauius. hence: home you idle creatures, get you home:\n",
      "is this a holiday? what, know you not\n",
      "(being mechanicall) you ought not walke\n",
      "vpon a labouring day, without the signe\n",
      "of your profession? speake, what trade art thou?\n",
      "  car. why sir, a carpenter\n",
      "\n",
      "   mur. where is thy leather apron, and thy rule?\n",
      "what dost thou with thy best apparrell on\n"
     ]
    }
   ],
   "source": [
    "# Note, takes into account the form only with print() function\n",
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Create a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences: 125168\n"
     ]
    }
   ],
   "source": [
    "# Here you create overlapping samples of text of length 40 characters (starting from character positions 0,3,6,9,...)\n",
    "maxlen=40\n",
    "step=3\n",
    "sentences=[]\n",
    "next_chars=[]\n",
    "for i in range(0,len(text)-maxlen,step):\n",
    "    sentences.append(text[i:i+maxlen])  # 40 character length sequences, taken from every 3rd character\n",
    "    next_chars.append(text[i+maxlen])  # 41st character for each sequence\n",
    "print('number of sequences:',len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) One-hot encode the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.zeros((len(sentences),maxlen,len(chars)),dtype=np.bool)\n",
    "y=np.zeros((len(sentences),len(chars)),dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t,char in enumerate(sentence):\n",
    "        X[i,t,char_indices[char]]=1\n",
    "    y[i,char_indices[next_chars[i]]]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Create character-based LSTM model for generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons=128\n",
    "model=Sequential([\n",
    "    LSTM(num_neurons,  # return_sequences=False is the default -> we want output only at last timestep\n",
    "        input_shape=(maxlen,len(chars))),  # length of sequences * length of one-hot encoding\n",
    "    # Flatten(), # Here we don't need flatten layer since the output comes only from last timestep, thus its shape is num_neurons\n",
    "    Dense(len(chars),activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy']) # If binary classes: loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=6\n",
    "batch_size=128\n",
    "model_structure=model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125168 samples\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 117s 932us/sample - loss: 2.0620 - accuracy: 0.3918\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 110s 879us/sample - loss: 1.6980 - accuracy: 0.4886\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 109s 868us/sample - loss: 1.5897 - accuracy: 0.5166\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 112s 893us/sample - loss: 1.5266 - accuracy: 0.5355\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 108s 862us/sample - loss: 1.4817 - accuracy: 0.5464\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 113s 905us/sample - loss: 1.4483 - accuracy: 0.5548\n",
      "Train on 125168 samples\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 112s 891us/sample - loss: 1.4224 - accuracy: 0.5624\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 111s 889us/sample - loss: 1.4006 - accuracy: 0.5688\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 114s 913us/sample - loss: 1.3834 - accuracy: 0.5734\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 111s 885us/sample - loss: 1.3696 - accuracy: 0.5763\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 109s 873us/sample - loss: 1.3602 - accuracy: 0.5796\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 104s 833us/sample - loss: 1.3463 - accuracy: 0.5831\n",
      "Train on 125168 samples\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 109s 873us/sample - loss: 1.3373 - accuracy: 0.5850\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 107s 857us/sample - loss: 1.3276 - accuracy: 0.5865\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 109s 869us/sample - loss: 1.3176 - accuracy: 0.5904\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 116s 925us/sample - loss: 1.3125 - accuracy: 0.5922\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 110s 879us/sample - loss: 1.3080 - accuracy: 0.5934\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 111s 888us/sample - loss: 1.3011 - accuracy: 0.5945\n",
      "Train on 125168 samples\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 108s 862us/sample - loss: 1.2962 - accuracy: 0.5974\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 115s 915us/sample - loss: 1.2891 - accuracy: 0.5980\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 109s 869us/sample - loss: 1.2826 - accuracy: 0.5995\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 108s 865us/sample - loss: 1.2793 - accuracy: 0.6005\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 106s 848us/sample - loss: 1.2742 - accuracy: 0.6029\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 115s 915us/sample - loss: 1.2701 - accuracy: 0.6031\n",
      "Train on 125168 samples\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 108s 866us/sample - loss: 1.2650 - accuracy: 0.6042\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 114s 911us/sample - loss: 1.2601 - accuracy: 0.6066\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 111s 889us/sample - loss: 1.2565 - accuracy: 0.6062\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 109s 871us/sample - loss: 1.2516 - accuracy: 0.6076\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 111s 884us/sample - loss: 1.2504 - accuracy: 0.6093\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 115s 921us/sample - loss: 1.2463 - accuracy: 0.6105\n"
     ]
    }
   ],
   "source": [
    "# Train for a while, then save the model, then continue training. (it continue where it ended last time)\n",
    "# Alternative method is to use callback functions from Keras.\n",
    "with open(\"shakes_lstm_model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_structure)  # this only saves the structure, not the weights\n",
    "np.random.seed(1337)\n",
    "for i in range(5):\n",
    "    model.fit(X,y,batch_size=batch_size, epochs=epochs)\n",
    "    model.save_weights(\"shakes_lstm_weights_{}.h5\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Create a sample to generate character sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM network output predictions (probability for all characters)\n",
    "\n",
    "Let's create a function that generates (draws from the prob.distribution) the next character based on those probabilities.\n",
    "- here dividing by the temperature is flattening (if >1) or sharpening (if <1) the prob.distribution\n",
    "- temperature (or diversity) <1 creates text that is very close to original\n",
    "- temperature (or diversity) >1 creates more diverse type of outcome.\n",
    "\n",
    "Numpy random function np.random.multinomial(num_samples,probab_list,size)\n",
    "- makes num_samples from the distribution given in the probab_list.\n",
    "- it outputs a list of length size which is equal to the number of experiments.\n",
    "- here there is only one experiment, i.e. only one output is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(preds,temperature=1.0):\n",
    "    preds=np.asarray(preds).astype('float64')\n",
    "    preds=np.log(preds)/temperature\n",
    "    exp_preds=np.exp(preds)\n",
    "    preds=exp_preds/np.sum(exp_preds) # This is softmax\n",
    "    probas=np.random.multinomial(1,preds,1) # Multinomial draw from probabilities\n",
    "    return np.argmax(probas) # return the class that was drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) Generate three texts with three diversity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------diversity: 0.2\n",
      "-----------Generating with seed:\"france of the best ranck and station,\n",
      "ar\"\n",
      "france of the best ranck and station,\n",
      "are ankellish then the world to the common the selfe,\n",
      "and the selfe and fierie to me to the sould,\n",
      "the face to the common the soule be well,\n",
      "that i will not be a will him to the street,\n",
      "that i will see the death, the selfe to the compost,\n",
      "that i will see the world to the selfe, and there\n",
      "i shall be so be so father: the selfe the street,\n",
      "that i will be sonne of men, and with the man of the fathers co\n",
      "\n",
      "-----------diversity: 0.5\n",
      "-----------Generating with seed:\"france of the best ranck and station,\n",
      "ar\"\n",
      "france of the best ranck and station,\n",
      "are like a good of the boben, that i haue\n",
      "their fire as chome dismitted enterping ang\n",
      "ye manney to thy thing, i to make a creat,\n",
      "as thou my lord, i haue not be a bone,\n",
      "that i haue me and thing, and then my lord?\n",
      "  os. they in him, you may well and honor, will plucke them bend,\n",
      "that should be a countence of ites to the all\n",
      "is is the death, i must caesar, and but his house,\n",
      "mee shall she now returne, \n",
      "\n",
      "-----------diversity: 1.0\n",
      "-----------Generating with seed:\"france of the best ranck and station,\n",
      "ar\"\n",
      "france of the best ranck and station,\n",
      "are tell your recrie: i are if thy loat my hours;\n",
      "thwoat sille\n",
      "\n",
      "   ham. stander siton ine sweetarne,\n",
      "and os't, and viui, cut. that i more such souldeding yet heare on?\n",
      "yet all he procloring, and yet sheye,\n",
      "and that i sent of her cool'd: so hats brutus,\n",
      "you are deitinich, my lord, heary you hold there hang on thats.\n",
      "come antony, men say, i did e-lw rash loo?\n",
      "govet yet is then your sode: or end. the r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "start_index=random.randint(0,len(text)-maxlen-1)\n",
    "for diversity in [0.2,0.5,1.0]:\n",
    "    print()\n",
    "    print('-----------diversity:',diversity)\n",
    "    generated=''\n",
    "    sentence=text[start_index:start_index+maxlen]\n",
    "    generated+=sentence\n",
    "    print('-----------Generating with seed:\"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    for i in range(400):\n",
    "        x=np.zeros((1,maxlen,len(chars)))\n",
    "        for t,char in enumerate(sentence):\n",
    "            x[0,t,char_indices[char]]=1\n",
    "        preds=model.predict(x,verbose=0)[0] # create prediction (proba_list)\n",
    "        next_index=sample(preds,diversity) # draw the index of next character\n",
    "        next_char=indices_char[next_index] # check what is the character\n",
    "        generated+=next_char\n",
    "        sentence=sentence[1:]+next_char # create 40-character seed for next round of character prediction\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush() # flushes the internal buffer to the console so next char appears immediately\n",
    "    print()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the generated text:\n",
    "- Diversity 0.2 and 0.5 both give something that looks like Shakespear.\n",
    "- Diversity 1.0 (with this dataset) starts to go off the rails fairly quickly.\n",
    "\n",
    "How to improve the model:\n",
    "- use larger corpus\n",
    "- use larger number of neurons\n",
    "- segment sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Other type of cell: GRU (gated recurrent unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The syntax is exactly the same as with LSTM\n",
    "# GRU(num_neurons,return_sequences=True,input_shape=X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j) Several layers of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have several layers of LSTM, ie creating a deeper LSTM network\n",
    "- then return_sequences=True is essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of deep LSTM with two layers\n",
    "num_neurons2=128\n",
    "model_deep=Sequential([\n",
    "    LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape),  \n",
    "    LSTM(num_neurons2, return_sequences=True), \n",
    "    Dense(len(chars),activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however that creating complex model\n",
    "- that is capable of representing more complex relationships than are present in the data can lead to strange results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
