{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "NLP_6_LSTM_gener_charseq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvI_sShdKbQw",
        "colab_type": "text"
      },
      "source": [
        "# NLP for text generation (char based)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olOFJ2qzKbSk",
        "colab_type": "text"
      },
      "source": [
        "In this notebook NLP is experimented with long short term memory units, LSTMs. \n",
        "\n",
        "Tasks performed with LSTMs:\n",
        "- Generating text รก la Shakespeare (with character based model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2z3vl6IKbTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.width=120\n",
        "#pd.set_option('display.width',75)\n",
        "#pd.options.display.max_columns=8\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import nlpia\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize.casual import casual_tokenize\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import copy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPoi0BbDKbWA",
        "colab_type": "text"
      },
      "source": [
        "The alternatives are as below, let's use tf.keras here.\n",
        "- multibackend Keras \n",
        "- tf.keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUIQNl4hKbWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "ee337aea-4117-4f3b-dbdf-79dda774dfb6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9AZa6JGKbX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d79b9b79-77e3-4c57-a8b1-326f155bb1dc"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnsAU5ohKbZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEne9cRtKbb9",
        "colab_type": "text"
      },
      "source": [
        "## Generating Shakespearean text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX3DHYxDKbcW",
        "colab_type": "text"
      },
      "source": [
        "The idea in the task \n",
        "- to learn to predict 41st character, based on 40 characters that came before that.\n",
        "- for this purpose the source text is chopped up into data samples, each with the fixed size of 40 characters.\n",
        "- the samples are taken as follows: take 40 chars from beginning, then move to 3rd character, take 40 from there etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-W3qyyKbc-",
        "colab_type": "text"
      },
      "source": [
        "### a) Load Project Gutenberg dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ17B8TgNVO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import gutenberg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB6pTk_cPfIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "03d3d840-d5ab-4b5d-b4ed-09f3ba3b97f9"
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7k9ABuZNfY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "a29f75f8-5d4a-427c-ecea-8780fd568d8c"
      },
      "source": [
        "gutenberg.fileids()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8xUaq1SKbgP",
        "colab_type": "text"
      },
      "source": [
        "### b) Load three Shakespeare texts and preprocess them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KRe4GSQKbgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "796014f5-476c-4a94-9978-f12f58e08d3f"
      },
      "source": [
        "text=''\n",
        "for txt in gutenberg.fileids():\n",
        "    if 'shakespeare' in txt:\n",
        "        text+=gutenberg.raw(txt).lower()  # load the raw text based on the selected fileid and concatenate it to text string\n",
        "chars=sorted(list(set(text)))  # create a set of all characters, turn it into a list, and then sort it.\n",
        "char_indices=dict((c,i) for i,c in enumerate(chars))  \n",
        "indices_char=dict((i,c) for i,c in enumerate(chars))\n",
        "'corpus length: {} total chars: {}'.format(len(text),len(chars))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus length: 375542 total chars: 50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRhJh4YFKbhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "f8d5c119-cc8a-44f2-af7d-ef1065871716"
      },
      "source": [
        "# Note, takes into account the form only with print() function\n",
        "print(text[0:500])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[the tragedie of julius caesar by william shakespeare 1599]\n",
            "\n",
            "\n",
            "actus primus. scoena prima.\n",
            "\n",
            "enter flauius, murellus, and certaine commoners ouer the stage.\n",
            "\n",
            "  flauius. hence: home you idle creatures, get you home:\n",
            "is this a holiday? what, know you not\n",
            "(being mechanicall) you ought not walke\n",
            "vpon a labouring day, without the signe\n",
            "of your profession? speake, what trade art thou?\n",
            "  car. why sir, a carpenter\n",
            "\n",
            "   mur. where is thy leather apron, and thy rule?\n",
            "what dost thou with thy best apparrell on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUgBBlQCKbip",
        "colab_type": "text"
      },
      "source": [
        "### c) Create a training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWc1A1Y8KbjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b09e75e-29b1-4078-94cb-ea84962ff99f"
      },
      "source": [
        "# Create overlapping samples of text of length 40 characters (starting from character positions 0,3,6,9,...)\n",
        "maxlen=40\n",
        "step=3\n",
        "sentences=[]\n",
        "next_chars=[]\n",
        "for i in range(0,len(text)-maxlen,step):\n",
        "    sentences.append(text[i:i+maxlen])  # 40 character length sequences, taken from every 3rd character\n",
        "    next_chars.append(text[i+maxlen])  # 41st character for each sequence\n",
        "print('number of sequences:',len(sentences))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sequences: 125168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqertrT6KblG",
        "colab_type": "text"
      },
      "source": [
        "### d) One-hot encode the training samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c36LKgrKblY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=np.zeros((len(sentences),maxlen,len(chars)),dtype=np.bool)\n",
        "y=np.zeros((len(sentences),len(chars)),dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t,char in enumerate(sentence):\n",
        "        X[i,t,char_indices[char]]=1\n",
        "    y[i,char_indices[next_chars[i]]]=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hpsyhUQKbmi",
        "colab_type": "text"
      },
      "source": [
        "### e) Create character-based LSTM model for generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_86l5D6KbnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "f19d0d74-7d3d-40e5-8ef4-c79fcfe6a1fe"
      },
      "source": [
        "num_neurons=128\n",
        "model=Sequential([\n",
        "    LSTM(num_neurons,  # return_sequences=False is the default -> we want output only at last timestep\n",
        "        input_shape=(maxlen,len(chars))),  # length of sequences * length of one-hot encoding\n",
        "    # Flatten(), # Here we don't need flatten layer since the output comes only from last timestep, thus its shape is num_neurons\n",
        "    Dense(len(chars),activation=\"softmax\")  \n",
        "])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yTVhym3KboG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy']) # If binary classes: loss='binary_crossentropy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P6cuzvLKbpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "179d0007-3c5f-4543-d26e-83bc5fcc5534"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               91648     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                6450      \n",
            "=================================================================\n",
            "Total params: 98,098\n",
            "Trainable params: 98,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZOwklNXKbp5",
        "colab_type": "text"
      },
      "source": [
        "### f) Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZCYTw1sKbqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=6\n",
        "batch_size=128\n",
        "model_structure=model.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5SVkMcEKbq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a86ca0a-c43d-4de2-ee3a-6a8b27df1184"
      },
      "source": [
        "# Train for a while, then save the model, then continue training. (it continues where it ended last time)\n",
        "# Alternative method is to use callback functions from Keras.\n",
        "with open(\"shakes_lstm_model.json\",\"w\") as json_file:\n",
        "    json_file.write(model_structure)  # this only saves the structure, not the weights\n",
        "np.random.seed(1337)\n",
        "for i in range(5):\n",
        "    model.fit(X,y,batch_size=batch_size, epochs=epochs)\n",
        "    model.save_weights(\"shakes_lstm_weights_{}.h5\".format(i+1))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 125168 samples\n",
            "Epoch 1/6\n",
            "125168/125168 [==============================] - 78s 626us/sample - loss: 2.0875 - acc: 0.3853\n",
            "Epoch 2/6\n",
            "125168/125168 [==============================] - 76s 610us/sample - loss: 1.7223 - acc: 0.4821\n",
            "Epoch 3/6\n",
            "125168/125168 [==============================] - 77s 613us/sample - loss: 1.6164 - acc: 0.5116\n",
            "Epoch 4/6\n",
            "125168/125168 [==============================] - 76s 606us/sample - loss: 1.5545 - acc: 0.5283\n",
            "Epoch 5/6\n",
            "125168/125168 [==============================] - 76s 606us/sample - loss: 1.5162 - acc: 0.5382\n",
            "Epoch 6/6\n",
            "125168/125168 [==============================] - 76s 606us/sample - loss: 1.4885 - acc: 0.5462\n",
            "Train on 125168 samples\n",
            "Epoch 1/6\n",
            "125168/125168 [==============================] - 76s 609us/sample - loss: 1.8279 - acc: 0.5490\n",
            "Epoch 2/6\n",
            "125168/125168 [==============================] - 76s 608us/sample - loss: 2.5111 - acc: 0.5264\n",
            "Epoch 3/6\n",
            "125168/125168 [==============================] - 76s 604us/sample - loss: 1.4634 - acc: 0.5519\n",
            "Epoch 4/6\n",
            "125168/125168 [==============================] - 76s 604us/sample - loss: 1.4373 - acc: 0.5599\n",
            "Epoch 5/6\n",
            "125168/125168 [==============================] - 75s 599us/sample - loss: 1.4227 - acc: 0.5622\n",
            "Epoch 6/6\n",
            "125168/125168 [==============================] - 74s 591us/sample - loss: 1.4090 - acc: 0.5656\n",
            "Train on 125168 samples\n",
            "Epoch 1/6\n",
            "125168/125168 [==============================] - 74s 589us/sample - loss: 1.3999 - acc: 0.5672\n",
            "Epoch 2/6\n",
            "125168/125168 [==============================] - 74s 595us/sample - loss: 1.3938 - acc: 0.5693\n",
            "Epoch 3/6\n",
            "125168/125168 [==============================] - 75s 601us/sample - loss: 1.3849 - acc: 0.5730\n",
            "Epoch 4/6\n",
            "125168/125168 [==============================] - 75s 599us/sample - loss: 1.3778 - acc: 0.5737\n",
            "Epoch 5/6\n",
            "125168/125168 [==============================] - 75s 596us/sample - loss: 1.3728 - acc: 0.5753\n",
            "Epoch 6/6\n",
            "125168/125168 [==============================] - 74s 592us/sample - loss: 1.3686 - acc: 0.5767\n",
            "Train on 125168 samples\n",
            "Epoch 1/6\n",
            "125168/125168 [==============================] - 74s 591us/sample - loss: 1.3621 - acc: 0.5776\n",
            "Epoch 2/6\n",
            "125168/125168 [==============================] - 75s 597us/sample - loss: 1.3567 - acc: 0.5803\n",
            "Epoch 3/6\n",
            "125168/125168 [==============================] - 75s 596us/sample - loss: 1.3516 - acc: 0.5798\n",
            "Epoch 4/6\n",
            "125168/125168 [==============================] - 75s 596us/sample - loss: 1.3474 - acc: 0.5820\n",
            "Epoch 5/6\n",
            "125168/125168 [==============================] - 74s 592us/sample - loss: 1.3446 - acc: 0.5818\n",
            "Epoch 6/6\n",
            "125168/125168 [==============================] - 74s 591us/sample - loss: 1.3412 - acc: 0.5834\n",
            "Train on 125168 samples\n",
            "Epoch 1/6\n",
            "125168/125168 [==============================] - 74s 589us/sample - loss: 1.3354 - acc: 0.5843\n",
            "Epoch 2/6\n",
            "125168/125168 [==============================] - 73s 585us/sample - loss: 1.3363 - acc: 0.5840\n",
            "Epoch 3/6\n",
            "125168/125168 [==============================] - 74s 588us/sample - loss: 1.3313 - acc: 0.5858\n",
            "Epoch 4/6\n",
            "125168/125168 [==============================] - 74s 588us/sample - loss: 1.3302 - acc: 0.5862\n",
            "Epoch 5/6\n",
            "125168/125168 [==============================] - 73s 586us/sample - loss: 1.3270 - acc: 0.5875\n",
            "Epoch 6/6\n",
            "125168/125168 [==============================] - 73s 587us/sample - loss: 1.3255 - acc: 0.5876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geUUVLEyKbri",
        "colab_type": "text"
      },
      "source": [
        "### g) Create a sample to generate character sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCh_VRsWKbsV",
        "colab_type": "text"
      },
      "source": [
        "The LSTM network output predictions (probability for all characters)\n",
        "\n",
        "Let's create a function that generates (draws from the prob.distribution) the next character based on those probabilities.\n",
        "- here dividing by the temperature is flattening (if >1) or sharpening (if <1) the prob.distribution\n",
        "- temperature (or diversity) <1 creates text that is very close to original\n",
        "- temperature (or diversity) >1 creates more diverse type of outcome.\n",
        "\n",
        "Numpy random function np.random.multinomial(num_samples,probab_list,size)\n",
        "- makes num_samples from the distribution given in the probab_list.\n",
        "- it outputs a list of length size which is equal to the number of experiments.\n",
        "- here there is only one experiment, i.e. only one output is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4QB2TNrKbsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def sample(preds,temperature=1.0):\n",
        "    preds=np.asarray(preds).astype('float64')\n",
        "    preds=np.log(preds)/temperature\n",
        "    exp_preds=np.exp(preds)\n",
        "    preds=exp_preds/np.sum(exp_preds) # This is softmax\n",
        "    probas=np.random.multinomial(1,preds,1) # Multinomial draw from probabilities\n",
        "    return np.argmax(probas) # return the class that was drawn."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lwWRr0FKbtY",
        "colab_type": "text"
      },
      "source": [
        "### h) Generate three texts with three diversity levels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOl6jPZpKbuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "61fcb5a1-f2c9-45e5-957f-5f719cc373f1"
      },
      "source": [
        "import sys\n",
        "start_index=random.randint(0,len(text)-maxlen-1)\n",
        "for diversity in [0.2,0.5,1.0]:\n",
        "    print()\n",
        "    print('-----------diversity:',diversity)\n",
        "    generated=''\n",
        "    sentence=text[start_index:start_index+maxlen]\n",
        "    generated+=sentence\n",
        "    print('-----------Generating with seed:\"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    for i in range(400):\n",
        "        x=np.zeros((1,maxlen,len(chars)))\n",
        "        for t,char in enumerate(sentence):\n",
        "            x[0,t,char_indices[char]]=1\n",
        "        preds=model.predict(x,verbose=0)[0] # create prediction (proba_list)\n",
        "        next_index=sample(preds,diversity) # draw the index of next character\n",
        "        next_char=indices_char[next_index] # check what is the character\n",
        "        generated+=next_char\n",
        "        sentence=sentence[1:]+next_char # create 40-character seed for next round of character prediction\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush() # flushes the internal buffer to the console so next char appears immediately\n",
        "    print()       "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------diversity: 0.2\n",
            "-----------Generating with seed:\",\n",
            "chaplesse, and knockt about the mazard\"\n",
            ",\n",
            "chaplesse, and knockt about the mazard\n",
            "they of his speake to him a so?\n",
            "  ham. i hau"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "e not the triffe in the streetes and come sould\n",
            "it one of him of his heares,\n",
            "and speake too't the speake to him a so?\n",
            "  cask. what shew not the world, and the propsing speech,\n",
            "to speake to him a sould me the end the proppee\n",
            "it selfe shall be a charmes of me, and the end the triffe\n",
            "he with the strong to the strong to the strong the cuterers to the strong\n",
            "\n",
            "-----------diversity: 0.5\n",
            "-----------Generating with seed:\",\n",
            "chaplesse, and knockt about the mazard\"\n",
            ",\n",
            "chaplesse, and knockt about the mazard\n",
            "i am to the rounded the enorcher they passes this\n",
            "the capt of the indeed him a words,\n",
            "and time to the dead speake of the prophely\n",
            "hand hath much and be face and caesar?\n",
            "  hor. send of a man, and the many man apprillaile,\n",
            "and the remember with a chariokne vs, if i liue of hee\n",
            "a weare the best the smiles of match,\n",
            "and so thinke he sirs-beauer that i should not growne\n",
            "beare their store to his bones \n",
            "\n",
            "-----------diversity: 1.0\n",
            "-----------Generating with seed:\",\n",
            "chaplesse, and knockt about the mazard\"\n",
            ",\n",
            "chaplesse, and knockt about the mazard\n",
            "\n",
            "   ham. so i would speake my spaies, and enterprse,\n",
            "to looknortre in at night.\n",
            "\n",
            "exit\n",
            "\n",
            "  bru. goodnidly noble senquo.\n",
            "  laer. that she to haue i makes hazait on octauiuus,\n",
            "calke ouer to thee thets of state tell romay they\n",
            "the the trutes in a spiritessed thates to deeuries,\n",
            "he had not leaue him on our stroone make.\n",
            "and shall haue that tromp'd good and mightly,\n",
            "not barlolding in byrding in surments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKb2GE8yKbu3",
        "colab_type": "text"
      },
      "source": [
        "Evaluation of the generated text:\n",
        "- Diversity 0.2 and 0.5 both give something that looks like Shakespear.\n",
        "- Diversity 1.0 (with this dataset) starts to go off the rails fairly quickly.\n",
        "\n",
        "How to improve the model:\n",
        "- use larger corpus\n",
        "- use larger number of neurons\n",
        "- segment sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4wQtsbLKbu_",
        "colab_type": "text"
      },
      "source": [
        "### i) Other type of cell: GRU (gated recurrent unit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEjUvSpcKbvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The syntax is exactly the same as with LSTM\n",
        "# GRU(num_neurons,return_sequences=True,input_shape=X[0].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AwHQLL2KbwP",
        "colab_type": "text"
      },
      "source": [
        "### j) Several layers of LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrX82xgtKbwe",
        "colab_type": "text"
      },
      "source": [
        "You can have several layers of LSTM, ie creating a deeper LSTM network\n",
        "- then return_sequences=True is essential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6loV6wPCKbws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of deep LSTM with two layers\n",
        "num_neurons2=128\n",
        "model_deep=Sequential([\n",
        "    LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape),  \n",
        "    LSTM(num_neurons2, return_sequences=True), \n",
        "    Dense(len(chars),activation=\"softmax\")  \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-2LG_FjKbyA",
        "colab_type": "text"
      },
      "source": [
        "Note, however that creating complex model\n",
        "- that is capable of representing more complex relationships than are present in the data can lead to strange results."
      ]
    }
  ]
}